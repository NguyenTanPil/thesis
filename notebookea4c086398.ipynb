{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\MyPC\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","import string\n","import tensorflow as tf\n","import tensorflow_datasets as tfds\n","import pickle\n","import time\n","import string\n","import re\n","from pyvi import ViTokenizer\n","import transformers\n","import unidecode\n"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["import logging\n","\n","logger = logging.getLogger(__name__)\n","\n","f_handler = logging.FileHandler('./train_model.log')\n","f_format = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n","f_handler.setFormatter(f_format)\n","logger.addHandler(f_handler)"]},{"cell_type":"code","execution_count":3,"metadata":{"trusted":true},"outputs":[],"source":["\n","intab_l = \"ạảãàáâậầấẩẫăắằặẳẵóòọõỏôộổỗồốơờớợởỡéèẻẹẽêếềệểễúùụủũưựữửừứíìịỉĩýỳỷỵỹđ\"\n","ascii_lowercase = 'abcdefghijklmnopqrstuvwxyz'\n","digits = '0123456789'\n","punctuation = r\"\"\"!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\"\"\"\n","whitespace = ' '\n","accept_strings =  intab_l + ascii_lowercase + digits + punctuation + whitespace\n","r = re.compile('^[' + accept_strings + ']+$')\n","\n","\n","# Một câu sẽ được coi là hợp lệ nếu có các ký tự nằm trong accept_strings\n","def _check_tieng_viet(seq):\n","    if re.match(r, seq.lower()):\n","        return True\n","    else:\n","        return False"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["def remove_punctuation(txt):\n","    txt = txt.translate(str.maketrans('', '', string.punctuation))\n","    return txt"]},{"cell_type":"code","execution_count":5,"metadata":{"trusted":true},"outputs":[{"data":{"text/plain":["'Trên co sở kết quả kiem tra hien trang'"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["def remove_random_accent(text, ratio=1): \n","    words = text.split()\n","    mask = np.random.random(size=len(words)) < ratio\n","    for i in range(len(words)):\n","        if mask[i]:\n","            words[i] = unidecode.unidecode(words[i])\n","    return ' '.join(words)\n","chars_regrex = '[aàảãáạăằẳẵắặâầẩẫấậAÀẢÃÁẠĂẰẲẴẮẶÂẦẨẪẤẬoòỏõóọôồổỗốộơờởỡớợOÒỎÕÓỌÔỒỔỖỐỘƠỜỞỠỚỢeèẻẽéẹêềểễếệEÈẺẼÉẸÊỀỂỄẾỆuùủũúụưừửữứựUÙỦŨÚỤƯỪỬỮỨỰiìỉĩíịIÌỈĨÍỊyỳỷỹýỵYỲỶỸÝỴnNvVmMCG]'\n","same_chars = { 'a': ['á', 'à', 'ả', 'ã', 'ạ', 'ấ', 'ầ', 'ẩ', 'ẫ', 'ậ', 'ắ', 'ằ', 'ẳ', 'ẵ', 'ặ'], 'A': ['Á','À','Ả','Ã','Ạ','Ấ','Ầ','Ẩ','Ẫ','Ậ','Ắ','Ằ','Ẳ','Ẵ','Ặ'], 'O': ['Ó','Ò','Ỏ','Õ','Ọ','Ô','Ố','Ồ','Ổ','Ỗ','Ộ','Ơ','Ớ','Ờ','Ở','Ỡ','Ợ','Q'], 'o': ['ó', 'ò', 'ỏ', 'õ', 'ọ', 'ô', 'ố', 'ồ', 'ổ', 'ỗ', 'ộ', 'ơ','ớ', 'ờ', 'ở', 'ỡ', 'ợ', 'q'], 'e': ['é', 'è', 'ẻ', 'ẽ', 'ẹ', 'ế', 'ề', 'ể', 'ễ', 'ệ', 'ê'], 'E': ['É', 'È', 'Ẻ', 'Ẽ', 'Ẹ', 'Ế', 'Ề', 'Ể', 'Ễ', 'Ệ', 'Ê'], 'u': ['ú', 'ù', 'ủ', 'ũ', 'ụ', 'ứ', 'ừ', 'ử', 'ữ', 'ự', 'ư'], 'U': ['Ú', 'Ù', 'Ủ', 'Ũ', 'Ụ', 'Ứ', 'Ừ', 'Ử', 'Ữ', 'Ự', 'Ư'], 'i': ['í', 'ì', 'ỉ', 'ĩ', 'ị'], 'I': ['Í', 'Ì', 'Ỉ', 'Ĩ', 'Ị'], 'y': ['ý', 'ỳ', 'ỷ', 'ỹ', 'ỵ', 'v'], 'Y': ['Ý', 'Ỳ', 'Ỷ', 'Ỹ', 'Ỵ', 'V'], 'n': ['m'], 'N': ['N'], 'v': ['y'], 'V': ['Y'], 'm': ['n'], 'M': ['N'], 'C': ['G'], 'G': ['C']\n","}\n","def _char_regrex(text):\n","    match_chars = re.findall(chars_regrex, text)\n","    return match_chars \n","def _random_replace(text, match_chars):\n","    replace_char = match_chars[np.random.randint(low=0, high=len(match_chars), size=1)[0]] \n","    insert_chars = same_chars[unidecode.unidecode(replace_char)] \n","    insert_char = insert_chars[np.random.randint(low=0, high=len(insert_chars), size=1)[0]] \n","    text = text.replace(replace_char, insert_char, 1) \n","    return text \n","def change(text): \n","    match_chars = _char_regrex(text) \n","    if len(match_chars) == 0:\n","        return text \n","    text = _random_replace(text, match_chars) \n","    return text \n","def replace_accent_chars(text, ratio=0.15): \n","    words = text.split() \n","    mask = np.random.random(size=len(words)) < ratio \n","    for i in range(len(words)): \n","        if mask[i]: \n","            words[i] = change(words[i]) \n","            break \n","    return ' '.join(words)\n","sentences_error = remove_random_accent(\"Trên cơ sở kết quả kiểm tra hiện trạng\", 0.5)\n","# sentences_error = replace_accent_chars(sentences_error, 0.5)\n","sentences_error"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["data = pd.read_csv(\"./static/100k_sentences_data.csv\")\n","data['status'] = data['text'].apply(_check_tieng_viet)\n","data['text'] = data['text'].apply(remove_punctuation)\n","data = data[['status', 'text']]"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["_data = data[data['status'] == True]\n","_data = _data.sample(len(_data))"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["num_val_and_test = 15000 \n","\n","def split_train_test_val(data, num_val_test):\n","    #split test data\n","    test_output = data['text'].values.tolist()[ : num_val_and_test]\n","    test_index = data.index.tolist()[ : num_val_and_test]\n","    test_input = [remove_random_accent(text) for text in test_output]\n","    test = test_input, test_output, test_index\n","    #split val data\n","    val_output = data['text'].values.tolist()[num_val_and_test: 2 * num_val_and_test]\n","    val_index = data.index.tolist()[num_val_and_test: 2 * num_val_and_test]\n","    val_input = [remove_random_accent(text) for text in val_output]\n","    val = val_input, val_output, val_index\n","    #split train data\n","    train_output = data['text'].values.tolist()[2 * num_val_and_test: ]\n","    train_index = data.index.tolist()[2 * num_val_and_test:]\n","    train_input = [remove_random_accent(text) for text in train_output]\n","    train = train_input, train_output, train_index\n","    return train, test, val\n","\n","train, test, val = split_train_test_val(_data, num_val_and_test)\n","test_input, test_output, test_index = test\n","val_input, val_output, val_index = val\n","train_input, train_output, train_index = train"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["train_data = pd.DataFrame(train_input, columns={\"input\"})\n","train_data['output'] = train_output\n","train_data['index'] = train_index\n","val_data = pd.DataFrame(val_input, columns={\"input\"})\n","val_data['output'] = val_output\n","val_data['index'] = val_index\n","test_data = pd.DataFrame(test_input, columns={\"input\"})\n","test_data['output'] = test_output\n","test_data['index'] = test_index"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["train_data.to_csv(\"./static/train_data.csv\", index=False)\n","val_data.to_csv(\"./static/val_data.csv\", index=False)\n","test_data.to_csv(\"./static/test_data.csv\", index=False)"]},{"cell_type":"code","execution_count":6,"metadata":{"trusted":true},"outputs":[],"source":["train_data = pd.read_csv(\"./static/train_data.csv\")\n","train_input = train_data['input']\n","train_output = train_data['output']\n","train_index = train_data['index']\n","val_data = pd.read_csv(\"./static/val_data.csv\")\n","val_input = val_data['input']\n","val_output = val_data['output']\n","val_index = val_data['index']\n","test_data = pd.read_csv(\"./static/test_data.csv\")\n","test_input = test_data['input']\n","test_output = test_data['output']\n","test_index = test_data['index']"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[{"data":{"text/plain":["[]"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["tf.config.list_physical_devices(\"GPU\")"]},{"cell_type":"code","execution_count":7,"metadata":{"trusted":true},"outputs":[],"source":["train_examples = tf.data.Dataset.from_tensor_slices((train_input, train_output))\n","val_examples = tf.data.Dataset.from_tensor_slices((val_input, val_output))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["tokenizer_ipt = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n","    (ipt.numpy() for (ipt, opt) in train_examples), target_vocab_size=2**13)\n","\n","tokenizer_opt = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n","    (opt.numpy() for (ipt, opt) in train_examples), target_vocab_size=2**13)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def _save_pickle(path, obj):\n","    with open(path, 'wb') as f:\n","        pickle.dump(obj, f)\n","        \n","_save_pickle('./tokenizer_input.pkl', tokenizer_ipt)\n","_save_pickle('./tokenizer_output.pkl', tokenizer_opt)"]},{"cell_type":"code","execution_count":9,"metadata":{"trusted":true},"outputs":[],"source":["def _load_pickle(path):\n","    with open(path, 'rb') as f:\n","        obj = pickle.load(f)\n","    return obj\n","tokenizer_ipt = _load_pickle(\"./static/tokenizer_input.pkl\")\n","tokenizer_opt = _load_pickle(\"./static/tokenizer_output.pkl\")"]},{"cell_type":"code","execution_count":10,"metadata":{"trusted":true},"outputs":[],"source":["BUFFER_SIZE = 20000\n","BATCH_SIZE = 24\n","def encode(ipt, opt):\n","    ipt = [tokenizer_ipt.vocab_size] + tokenizer_ipt.encode(\n","        ipt.numpy()) + [tokenizer_ipt.vocab_size+1]\n","\n","    opt = [tokenizer_opt.vocab_size] + tokenizer_opt.encode(\n","        opt.numpy()) + [tokenizer_opt.vocab_size+1]\n","    return ipt, opt"]},{"cell_type":"code","execution_count":11,"metadata":{"trusted":true},"outputs":[],"source":["def tf_encode(ipt, opt):\n","    result_ipt, result_opt = tf.py_function(encode, [ipt, opt], [tf.int64, tf.int64])\n","    result_ipt.set_shape([None])\n","    result_opt.set_shape([None])\n","    return result_ipt, result_opt"]},{"cell_type":"code","execution_count":12,"metadata":{"trusted":true},"outputs":[],"source":["next(iter(train_examples))\n","MAX_LENGTH = 40\n","\n","def filter_max_length(x, y, max_length=MAX_LENGTH):\n","    return tf.logical_and(tf.size(x) <= max_length,\n","                        tf.size(y) <= max_length)\n","  \n","train_dataset = train_examples.map(tf_encode)\n","train_dataset = train_dataset.filter(filter_max_length)\n","# cache the dataset to memory to get a speedup while reading from it.\n","train_dataset = train_dataset.cache()\n","train_dataset = train_dataset.shuffle(BUFFER_SIZE).padded_batch(BATCH_SIZE)\n","train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n","\n","val_dataset = val_examples.map(tf_encode)\n","val_dataset = val_dataset.filter(filter_max_length).padded_batch(BATCH_SIZE)"]},{"cell_type":"code","execution_count":13,"metadata":{"trusted":true},"outputs":[],"source":["def get_angles(pos, i, d_model):\n","    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n","    return pos * angle_rates\n","\n","def positional_encoding(position, d_model):\n","    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n","                          np.arange(d_model)[np.newaxis, :],\n","                          d_model)\n","\n","    # apply sin to even indices in the array; 2i\n","    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n","\n","    # apply cos to odd indices in the array; 2i+1\n","    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n","\n","    pos_encoding = angle_rads[np.newaxis, ...]\n","\n","    return tf.cast(pos_encoding, dtype=tf.float32)\n","\n","def point_wise_feed_forward_network(d_model, dff):\n","    return tf.keras.Sequential([\n","      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n","      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n","    ])\n","\n","def scaled_dot_product_attention(q, k, v, mask):\n","    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n","\n","    # scale matmul_qk\n","    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n","    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n","\n","    # add the mask to the scaled tensor.\n","    if mask is not None:\n","        scaled_attention_logits += (mask * -1e9)\n","\n","    # softmax is normalized on the last axis (seq_len_k) so that the scores\n","    # add up to 1.\n","    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n","\n","    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n","\n","    return output, attention_weights\n","\n","def create_padding_mask(seq):\n","    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n","\n","    # add extra dimensions to add the padding\n","    # to the attention logits.\n","    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n","\n","def create_look_ahead_mask(size):\n","    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n","    return mask  # (seq_len, seq_len)\n","\n","def loss_function(real, pred):\n","    mask = tf.math.logical_not(tf.math.equal(real, 0))\n","    loss_ = loss_object(real, pred)\n","\n","    mask = tf.cast(mask, dtype=loss_.dtype)\n","    loss_ *= mask\n","\n","    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"]},{"cell_type":"code","execution_count":18,"metadata":{"_kg_hide-input":false,"_kg_hide-output":true,"trusted":true},"outputs":[],"source":["class MultiHeadAttention(tf.keras.layers.Layer):\n","    def __init__(self, d_model, num_heads):\n","        super(MultiHeadAttention, self).__init__()\n","        self.num_heads = num_heads\n","        self.d_model = d_model\n","\n","        assert d_model % self.num_heads == 0\n","\n","        self.depth = d_model // self.num_heads\n","\n","        self.wq = tf.keras.layers.Dense(d_model)\n","        self.wk = tf.keras.layers.Dense(d_model)\n","        self.wv = tf.keras.layers.Dense(d_model)\n","\n","        self.dense = tf.keras.layers.Dense(d_model)\n","        \n","    def split_heads(self, x, batch_size):\n","        \"\"\"Split the last dimension into (num_heads, depth).\n","        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n","        \"\"\"\n","        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n","        return tf.transpose(x, perm=[0, 2, 1, 3])\n","    \n","    def call(self, v, k, q, mask):\n","        batch_size = tf.shape(q)[0]\n","\n","        q = self.wq(q)  # (batch_size, seq_len, d_model)\n","        k = self.wk(k)  # (batch_size, seq_len, d_model)\n","        v = self.wv(v)  # (batch_size, seq_len, d_model)\n","\n","        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n","        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n","        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n","\n","        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n","        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n","        scaled_attention, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n","\n","        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n","\n","        concat_attention = tf.reshape(scaled_attention, \n","                                      (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n","\n","        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n","\n","        return output, attention_weights\n","\n","class EncoderLayer(tf.keras.layers.Layer):\n","    def __init__(self, d_model, num_heads, dff, rate=0.1):\n","        super(EncoderLayer, self).__init__()\n","\n","        self.mha = MultiHeadAttention(d_model, num_heads)\n","        self.ffn = point_wise_feed_forward_network(d_model, dff)\n","\n","        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","\n","        self.dropout1 = tf.keras.layers.Dropout(rate)\n","        self.dropout2 = tf.keras.layers.Dropout(rate)\n","\n","    def call(self, x, training, mask):\n","\n","        attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n","        attn_output = self.dropout1(attn_output, training=training)\n","        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n","\n","        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n","        ffn_output = self.dropout2(ffn_output, training=training)\n","        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n","\n","        return out2\n","    \n","class DecoderLayer(tf.keras.layers.Layer):\n","    def __init__(self, d_model, num_heads, dff, rate=0.1):\n","        super(DecoderLayer, self).__init__()\n","\n","        self.mha1 = MultiHeadAttention(d_model, num_heads)\n","        self.mha2 = MultiHeadAttention(d_model, num_heads)\n","\n","        self.ffn = point_wise_feed_forward_network(d_model, dff)\n","\n","        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","\n","        self.dropout1 = tf.keras.layers.Dropout(rate)\n","        self.dropout2 = tf.keras.layers.Dropout(rate)\n","        self.dropout3 = tf.keras.layers.Dropout(rate)\n","\n","    def call(self, x, enc_output, training,\n","           look_ahead_mask, padding_mask):\n","        # enc_output.shape == (batch_size, input_seq_len, d_model)\n","\n","        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n","        attn1 = self.dropout1(attn1, training=training)\n","        out1 = self.layernorm1(attn1 + x)\n","\n","        attn2, attn_weights_block2 = self.mha2(\n","            enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n","        attn2 = self.dropout2(attn2, training=training)\n","        out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n","\n","        ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n","        ffn_output = self.dropout3(ffn_output, training=training)\n","        out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n","\n","        return out3, attn_weights_block1, attn_weights_block2\n","    \n","class Encoder(tf.keras.layers.Layer):\n","    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n","               maximum_position_encoding, rate=0.1):\n","        super(Encoder, self).__init__()\n","\n","        self.d_model = d_model\n","        self.num_layers = num_layers\n","\n","        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n","        self.pos_encoding = positional_encoding(maximum_position_encoding,\n","                                                self.d_model)\n","\n","        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate)\n","                           for _ in range(num_layers)]\n","\n","        self.dropout = tf.keras.layers.Dropout(rate)\n","\n","    def call(self, x, training, mask):\n","\n","        seq_len = tf.shape(x)[1]\n","\n","        # adding embedding and position encoding.\n","        x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n","        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n","        x += self.pos_encoding[:, :seq_len, :]\n","\n","        x = self.dropout(x, training=training)\n","\n","        for i in range(self.num_layers):\n","            x = self.enc_layers[i](x, training, mask)\n","\n","        return x  # (batch_size, input_seq_len, d_model)\n","    \n","class Decoder(tf.keras.layers.Layer):\n","    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\n","               maximum_position_encoding, rate=0.1):\n","        super(Decoder, self).__init__()\n","\n","        self.d_model = d_model\n","        self.num_layers = num_layers\n","\n","        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n","        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n","\n","        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate)\n","                           for _ in range(num_layers)]\n","        self.dropout = tf.keras.layers.Dropout(rate)\n","\n","    def call(self, x, enc_output, training,\n","           look_ahead_mask, padding_mask):\n","\n","        seq_len = tf.shape(x)[1]\n","        attention_weights = {}\n","\n","        x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n","        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n","        x += self.pos_encoding[:, :seq_len, :]\n","\n","        x = self.dropout(x, training=training)\n","\n","        for i in range(self.num_layers):\n","            x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n","                                                 look_ahead_mask, padding_mask)\n","            attention_weights[f'decoder_layer{i+1}_block1'] = block1\n","            attention_weights[f'decoder_layer{i+1}_block2'] = block2\n","\n","        # x.shape == (batch_size, target_seq_len, d_model)\n","        return x, attention_weights\n","\n","class Transformer(tf.keras.Model):\n","    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, \n","               target_vocab_size, pe_input, pe_target, rate=0.1):\n","        super(Transformer, self).__init__()\n","\n","        self.encoder = Encoder(num_layers, d_model, num_heads, dff, \n","                               input_vocab_size, pe_input, rate)\n","\n","        self.decoder = Decoder(num_layers, d_model, num_heads, dff, \n","                               target_vocab_size, pe_target, rate)\n","\n","        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n","    \n","    def call(self, inp, tar, training, enc_padding_mask, \n","           look_ahead_mask, dec_padding_mask):\n","        # print('enc_padding_mask: ', enc_padding_mask)\n","        enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n","\n","        # dec_output.shape == (batch_size, tar_seq_len, d_model)\n","        dec_output, attention_weights = self.decoder(\n","            tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n","\n","        final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n","\n","        return final_output, attention_weights"]},{"cell_type":"markdown","metadata":{},"source":["HYPER PARAMETERS"]},{"cell_type":"markdown","metadata":{},"source":["num_layers = 4\n","d_model = 128\n","dff = 512\n","num_heads = 8\n","\n","input_vocab_size = tokenizer_ipt.vocab_size + 2\n","target_vocab_size = tokenizer_opt.vocab_size + 2\n","dropout_rate = 0.1\n","\n","----\n","\n","num_layers = 4\n","d_model = 64\n","dff = 256\n","num_heads = 8\n","\n","----\n","\n"]},{"cell_type":"code","execution_count":14,"metadata":{"trusted":true},"outputs":[],"source":["num_layers = 4\n","d_model = 128\n","dff = 512\n","num_heads = 8\n","\n","input_vocab_size = tokenizer_ipt.vocab_size + 2\n","target_vocab_size = tokenizer_opt.vocab_size + 2\n","dropout_rate = 0.1"]},{"cell_type":"code","execution_count":17,"metadata":{"trusted":true},"outputs":[],"source":["class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n","    def __init__(self, d_model, warmup_steps=4000):\n","        super(CustomSchedule, self).__init__()\n","\n","        self.d_model = d_model\n","        self.d_model = tf.cast(self.d_model, tf.float32)\n","\n","        self.warmup_steps = warmup_steps\n","    \n","    def __call__(self, step):\n","        arg1 = tf.math.rsqrt(step)\n","        arg2 = step * (self.warmup_steps ** -1.5)\n","\n","        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n","learning_rate = CustomSchedule(d_model)\n","\n","optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, \n","                                     epsilon=1e-9)\n","temp_learning_rate_schedule = CustomSchedule(d_model)\n","loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n","    from_logits=True, reduction='none')\n","def loss_function(real, pred):\n","    mask = tf.math.logical_not(tf.math.equal(real, 0))\n","    loss_ = loss_object(real, pred)\n","\n","    mask = tf.cast(mask, dtype=loss_.dtype)\n","    loss_ *= mask\n","\n","    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n","train_loss = tf.keras.metrics.Mean(name='train_loss')\n","train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n","    name='train_accuracy')"]},{"cell_type":"code","execution_count":19,"metadata":{"trusted":true},"outputs":[],"source":["transformer = Transformer(num_layers=num_layers, d_model=d_model, num_heads=num_heads, dff=dff,\n","                          input_vocab_size=input_vocab_size, target_vocab_size=target_vocab_size, \n","                          pe_input=input_vocab_size, \n","                          pe_target=target_vocab_size,\n","                          rate=dropout_rate)\n","def create_masks(inp, tar):\n","    # Encoder padding mask\n","    enc_padding_mask = create_padding_mask(inp)\n","\n","    # Used in the 2nd attention block in the decoder.\n","    # This padding mask is used to mask the encoder outputs.\n","    dec_padding_mask = create_padding_mask(inp)\n","\n","    # Used in the 1st attention block in the decoder.\n","    # It is used to pad and mask future tokens in the input received by \n","    # the decoder.\n","    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n","    dec_target_padding_mask = create_padding_mask(tar)\n","    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n","    return enc_padding_mask, combined_mask, dec_padding_mask"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# inp = tf.keras.Input(shape=(40))\n","# tar_inp = tf.keras.Input(shape=(40))\n","# enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n","# transformer(inp=inp, tar=tar_inp, training=True, enc_padding_mask=enc_padding_mask, look_ahead_mask=combined_mask, dec_padding_mask=dec_padding_mask)\n","# transformer.load_weights(\"../input/100k-sentences-data-vietnamese/model_weight_100e.h5\")"]},{"cell_type":"code","execution_count":20,"metadata":{"trusted":true},"outputs":[],"source":["checkpoint_path = \"./checkpoints/train_500k\"\n","\n","ckpt = tf.train.Checkpoint(transformer=transformer,\n","                           optimizer=optimizer)\n","\n","ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n","\n","# if a checkpoint exists, restore the latest checkpoint.\n","if ckpt_manager.latest_checkpoint:\n","    ckpt.restore(ckpt_manager.latest_checkpoint)\n","    print ('Latest checkpoint restored!!')"]},{"cell_type":"code","execution_count":21,"metadata":{"trusted":true},"outputs":[],"source":["train_step_signature = [\n","    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n","    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n","]\n","\n","@tf.function(input_signature=train_step_signature)\n","def train_step(inp, tar):\n","    tar_inp = tar[:, :-1]\n","    tar_real = tar[:, 1:]\n","    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n","    with tf.GradientTape() as tape:\n","        predictions, _ = transformer(inp, tar_inp, \n","                                     True, \n","                                     enc_padding_mask, \n","                                     combined_mask, \n","                                     dec_padding_mask)\n","\n","        loss = loss_function(tar_real, predictions)\n","\n","    gradients = tape.gradient(loss, transformer.trainable_variables)    \n","    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n","\n","    train_loss(loss)\n","    train_accuracy(tar_real, predictions)"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":false,"trusted":true},"outputs":[],"source":["EPOCHS = 200\n","for epoch in range(EPOCHS):\n","    start = time.time()\n","\n","    train_loss.reset_states()\n","    train_accuracy.reset_states()\n","\n","    # inp -> non_diacritic, tar -> diacritic\n","    for (batch, (inp, tar)) in enumerate(train_dataset):\n","        train_step(inp, tar)\n","        if batch % 50 == 0:\n","            logger.info('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(\n","              epoch + 1, batch, train_loss.result(), train_accuracy.result()))\n","\n","    if (epoch + 1) % 5 == 0:\n","        ckpt_save_path = ckpt_manager.save()\n","        logger.info('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n","                                                         ckpt_save_path))\n","\n","    logger.info('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1, \n","                                                train_loss.result(), \n","                                                train_accuracy.result()))\n","    print('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1, \n","                                                train_loss.result(), \n","                                                train_accuracy.result()))\n","    logger.info('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))\n","    print('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["transformer.save_weights(\"./model_weight.h5\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-10-20T15:42:36.920005Z","iopub.status.busy":"2022-10-20T15:42:36.919350Z","iopub.status.idle":"2022-10-20T15:42:36.954262Z","shell.execute_reply":"2022-10-20T15:42:36.953359Z","shell.execute_reply.started":"2022-10-20T15:42:36.919966Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","submisstion = pd.read_csv(\"../input/nlp-getting-started/sample_submission.csv\")\n","submisstion.to_csv(\"submisstion.csv\")"]}],"metadata":{"kernelspec":{"display_name":"Python 3.7.0 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.0"},"vscode":{"interpreter":{"hash":"a76c45e807eb28889890cb317abb47792c60e656125833f2a8c4eb3a697ddee5"}}},"nbformat":4,"nbformat_minor":4}
